# Activation Fucntion

앞서 DSC Yonsei 임성수 님께서 작성해 주신 Logistic Regression post에서 input과 가중치의 선형결합을 sigmoid 함수에 넣어서 output으로 이진분류를 하는 것으 배웠습니다.

또한 저의 지난 post인 ANN(Artificial Neural Network)에서는 MLP에서 각 노드들이 선형결합을 하고 activation function을 통해 노드의 input을 일종의 신호로 반환한다는 것을 배웠습니다.

따라서 이러한 post를 선험적으로 보는 것이 이번 포스트를 이해하는데 많은 도움이 될 수 있을 것이라고 생각합니다.

* [Logistic Regression](https://dscyonsei.tistory.com/4)
* [Artificial Neural Network](https://dscyonsei.tistory.com/3)

## 개요
우선 Activation Function과 같은 경우에는 MLP에서 input을 일종의 신호로 변환할 때 중요하게 작용합니다.
핵심적인 것으로는 숫자가 너무 크지 않아야 하고, 오차의 역전파 시 미분이 용이해야한다는 점이 중요합니다.

그 종류로는 다음고 같은 것들이 있고 이것들을 하나하나 따져가며 살펴봅시다.

1. Sigmoid function
2. ReLU function
3. Leacky ReLU function
4. GeLU function

## Sigmoid function
우선 대표적으로는 앞서 언급한 logistic regression에서 사용되는 Sigmoid function 입니다. 
시그모이드 함수는 그 그래프를 그렸을 때, 아래와 같이 0과 1사이의 값을 갖고 있고 $\phi(z) = {1 \over e^z}$의 식으로 표현됩니다.

그런데 이러한 sigmoid function의 식을 보면 하나 특이한 점이 있습니다. exponential function을 합성한 형태로 미분할 경우 너무 간단하 꼴이 도출된다는 점입니다.

따라서, MLP의 특성상 backpropagation을 써야하고, 그렇기 때문에 미분이 너무 복잡하면 안되고 이러한 관점에서 볼 때 활성화 함수로 용이합니다.

한 가지 단점은 layer가 더 깊이 쌓이는 모델에 backpropagation의 과정에 gradient가 0으로 수렴하는 gradient vanishing 문제가 발생한다는 것입니다..
이러한 단점은 너무나도 치명적이라서 최근 모델의 복잡도가 높아지는 추세에서는 잘 쓰이지 않는 편입니다.

## ReLU function
ReLU는 앞서 언급한 시그모이드 함수의 단점인 gradient vanishing 문제를 성공적으로 잘 해결한 활성화 함수입니다.
많은 논문들에서 해당 활성화 함수를 사용하고 있기도 합니다.

