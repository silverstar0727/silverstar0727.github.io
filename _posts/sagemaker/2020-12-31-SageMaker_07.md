---
layout: post
title: "Deploy TensorFlow Serving Models"
date: 2020-12-31
excerpt: "SageMaker- Train Model with a Tensorflow - Deploy TensorFlow Serving Models"
tags: [SageMaker]
comments: False
category: SageMaker
use_math: true
---

본 문서는 [AWS SageMaker의 official document](https://sagemaker.readthedocs.io/en/stable/frameworks/tensorflow/using_tf.html#train-a-model-with-tensorflow)를 토대로 작성되었습니다.

- - -

## Contents

#### Train a Model with TensorFlow
* [Prepare a Training Script](https://github.com/silverstar0727/AWS-SageMaker-Docs/blob/main/sagemaker/2020-12-26-SageMaker_01.md)(완)
  * Adapting your local TensorFlow script
  * Use third-party libraries
* [Create an Estimator](https://github.com/silverstar0727/AWS-SageMaker-Docs/blob/main/sagemaker/2020-12-27-SageMaker_02.md) (완)
  * Specify a Docker image using an Estimator
* [Call the fit Method](https://github.com/silverstar0727/AWS-SageMaker-Docs/blob/main/sagemaker/2020-12-28-SageMaker_03.md) (완)
* [Distributed Training](https://github.com/silverstar0727/AWS-SageMaker-Docs/blob/main/sagemaker/2020-12-29-SageMaker_04.md) (완)
  * Training with parameter servers
  * Training with Horovod
* [Training with Pipe Mode using PipeModeDataset](https://github.com/silverstar0727/AWS-SageMaker-Docs/blob/main/sagemaker/2020-12-30-SageMaker_05.md)(완)
* Training with MKL-DNN disabled (보류...)

#### Deploy TensorFlow Serving models
* [Deploy to a SageMaker Endpoint](https://github.com/silverstar0727/AWS-SageMaker-Docs/blob/main/sagemaker/2020-12-31-SageMaker_07.md) (완)
  * Deploying from an Estimator
    * What happens when deploy is called
  * Deploying directly from model artifacts
  * Making predictions against a SageMaker Endpoint
* Run a Batch Transform Job
  * Create a Transformer Object
  * Call transform
  * Batch Transform Supported Data Formats
* TensorFlow Serving Input and Output
  * Supported Formats
    * Simplified JSON Input
    * Line-delimited JSON
  * Create Python Scripts for Custom Input and Output Formats
    * How to implement the pre- and/or post-processing handler(s)
    
#### SageMaker TensorFlow Classes

#### SageMaker TensorFlow Docker containers

- - -


## Deploying to a SageMaker Endpoint
estimator.fit을 통해 학습이 끝나면, output_path에 정의된 S3 저장소에 저장된다. 이는 deploy메서드를 이용하여 endpoint를 만들 수 있다.
또는 transform을 호출하여 batch transform job을 실행하는 Transformer를 생성할 수 있다.
이러한 모델은 TensorFlow Serving 기반의 서버에 저장된다. 서버는 텐서플로 서빙 REST API를 제공한다.([REST API](https://www.tensorflow.org/tfx/serving/api_rest))

~~~python
from sagemaker.tensorflow import TensorFlow

estimator = TensorFlow(
    entry_point = 'tf-train.py',
    ...,
    instance_count = 1,
    instance_type = 'm1.c4.xlarge',
    framework_version = '2.2',
    py_version = 'py37',
    )

estimator.fit(inputs)
predictor = estimator.deploy(initial_intsance_count = 1, instance_type = 'ml.c5.xlarge')
~~~   

#### called 메서드를 호출하면 이루어지는 과정
* instance_type에서 지정한 EC2 인스턴스를 initial_instance_count만큼 실행한다
    * TensorFlow Serving에 최적화된 도커 컨테이너를 실행한다. ([도커 예제 깃헙](https://github.com/aws/sagemaker-tensorflow-serving-container))
    * 모델을 실행하도록 구성된 TensorFlow Serving 프로세스를 실행한다.
    * SageMaker InvokeEndpoint API를 통해서 TensorFlow Server에 엑세스를 제공하는 HTTP 서버를 시작한다.
    
배포가 끝나면 SageMaker Endpoint는 예측 요청에 대한 준비 상태가 된다. 

## Deploying directly from model artifacts

만약 기존의 모델 artifact가 있다면 교육과정을 건너 뛰고 endpoint에 배포할 수 있다.

또한, 이러한 추론과정을 가속화 하기 Elastic Accelerator를 제공한다. 이는 deploy에 accelerator_type 인자를 추가함으로써 사용할 수 있다.([Elastic 추론](https://docs.aws.amazon.com/sagemaker/latest/dg/ei.html))

~~~python
from sagemaker.tensorflow as TensorFlowModel

model = TensorFlowModel(model_data = 's3://mybucket/model.tar.gz', role = 'MySageMakerRole')
predictor = model.deploy(initial_instance_count = 1, instance_type = 'ml.c5.xlarge', accelerator_type = 'ml.eia1.medium')
~~~

## Making predictions against a SageMaker Endpoint

deploy메서드에 의해서 모델 인스턴스가 반환되면 endpoint에 대한 다음의 예제처럼 요청을 보낼 수 있다.

~~~python
input = {
    'intances': [1.0, 2.0, 5.0]
    }
    
result = predictor.predict(input)
~~~

위 코드를 통해 반환 받은 예측값은 다음과 같이 딕셔너리 형태로 반환된다.

{'predictions': [3.5, 4.0, 5.5]}

이러한 입출력 데이터 형식은 텐서플로 서빙 REST API에서의 Predict 메서드에 직접 대응한다.([REST API](tensorflow.org/tfx/serving/api_rest))

만약 저장된 모델이 signature_def를 포함하고 있다면 다음과 같이 분류나 회귀의 요청을 할 수도 있다. 
이때, 예측하고자 하는 값을 리스트에 dixtionary형태로 넣어서 examples의 키로 요청하게 된다.
predictor에서도 regress메서드를 이용하며, 결과값은 리스트의 형태로 반환된다.

~~~python
input = {
    'signature_name': 'tensorflow.serving/regress',
    'examples': [{'x': 1.0}, {'x': 2.0}]
    }
result = predictor.regress(input)
~~~

만일 다중 instances를 예측에 포함하거나, 여러 예측결과 고차원 배열로를 endpoint에 하나의 요청으로 가져올 수도 있다.
이는 개별 요청을 하는 것 보다 더 효율적이다.([](https://sagemaker.readthedocs.io/en/stable/frameworks/tensorflow/deploying_tensorflow_serving.html))

~~~python
input = {
  'instances': [
    [1.0, 2.0, 5.0],
    [1.0, 2.0, 5.0],
    [1.0, 2.0, 5.0]
  ]
}
result = predictor.predict(input)
~~~




