---
layout: post
title: "Hyperparameter tuning on sagemaker notebook"
date: 2021-02-10
excerpt: "Notebook에서 명령어를 활용한 하이퍼파라미터 튜닝 작업"
tags: [SageMaker]
comments: False
category: SageMaker
use_math: true
---

이번 포스트에서는 Notebook Instance에서 명령어를 통해 하이퍼파라미터 튜닝을 하는 방법에 대해서 배운다.

우선 시작에 앞서 하이퍼파라미터를 튜닝하는 작업에 대해 간략히 말하자면 다음과 같다. hyperparameter는 우리가 ml에서 흔히 접할 수 있는 parameter와 확연한 차이점을 가지고 있다.

parameter는 학습되는 가중치 및 bias를 의미하지만, hyperparameter는 layer의 개수는 몇개로 할 지, layer별 노드 개수는 몇개로 할 지, optimizer, loss function은 무엇을 쓰고
learning rate는 몇으로 할 지, epoch는 몇으로 할 지와 같은 우리가 ml에 입문하고 흔히 갖는 궁금한 부분일 것이다.

그런데, 이런 것들을 물어봐도 돌아오는 대답은 항상 같다. "나도 모른다."이다. ㅋㅋㅋㅋㅋㅋ 사실 예전엔 그걸 모르면 도대체 코드를 어떻게 짜야한단 말인가 하며 굉장히 난감해 했던 개인적인 경험들이 있다.

그리고 AI를 1년 반 가까이 공부하고 멘토링을 하며 많은 사람들이 나와 비슷한 고민을 하고 있다는 것을 알게 되었다.
따라서, 이번 기회에 하이퍼파라미터는 어떻게 결정되는가와 ML의 실험과 관련된 얘기를 조금 해보고자 본 포스트를 작성하게 되었다.ㄷ

포스트의 내용은 amazon sagemaker examples의 hpo 관련 repo를 참고하였고, 다음의 [링크](https://github.com/aws/amazon-sagemaker-examples/blob/master/hyperparameter_tuning/xgboost_direct_marketing/hpo_xgboost_direct_marketing_sagemaker_APIs.ipynb)에서 확인할 수 있다.

1. Libraries
2. Load Data
3. Preprocessing
4. Set tuning job config
5. Set training job definition
6. Launch the job

## Liraries
우선, 본 작업의 시작에 앞서 s3 버킷을 하나 생성해줘야 한다.

필요한 라이브러리들을 불러오고 다양한 사전 변수들을 지정해주자. 만약 아래의 코드가 이해가 잘 가지 않는다면 sagemaker와 관련된 본 블로그의 다양한 포스트를 사전에 보고 오자.
~~~
import sagemaker
import boto3

import numpy as np                                # For matrix operations and numerical processing
import pandas as pd                               # For munging tabular data
from time import gmtime, strftime                 
import os 
 
region = boto3.Session().region_name    
smclient = boto3.Session().client('sagemaker')

role = sagemaker.get_execution_role()

bucket = sagemaker.Session().default_bucket()
prefix = 'sagemaker/DEMO-hpo-xgboost-dm'
~~~

위 코드에서 마지막의 prefix는 본인의 s3 버킷의 이름을 지정하면된다. 가령, 본인이 s3에 'mybucket-1234'를 만들었다면 prefix = 'sagemaker/mybucket-1234'와 같은 형식이다.

## Load Data
데이터는 UCI의 ml repository의 direct marketing dataset을 활용하였다. 별도로 직접 다운 받을 필요 없이, 아래의 명령어를 통해 zip파일을 다운 받고 zip을 풀면 된다.
~~~
!wget -N https://archive.ics.uci.edu/ml/machine-learning-databases/00222/bank-additional.zip
!unzip -o bank-additional.zip
~~~

## Preprocessing
다양한 전처리 작업을 진행하게 되는데, 본 포스트에서 전처리를 주된 컨텐츠로 다루지 않기에 구체적인 설명은 생략하겠다.
~~~
data = pd.read_csv('./bank-additional/bank-additional-full.csv', sep=';')
pd.set_option('display.max_columns', 500)     # Make sure we can see all of the columns
pd.set_option('display.max_rows', 50)         # Keep the output on one page

data['no_previous_contact'] = np.where(data['pdays'] == 999, 1, 0)                                 
data['not_working'] = np.where(np.in1d(data['job'], ['student', 'retired', 'unemployed']), 1, 0) 
model_data = pd.get_dummies(data)                                                                  

model_data = model_data.drop(['duration', 'emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'nr.employed'], axis=1)

train_data, validation_data, test_data = np.split(model_data.sample(frac=1, random_state=1729), [int(0.7 * len(model_data)), int(0.9*len(model_data))])  

pd.concat([train_data['y_yes'], train_data.drop(['y_no', 'y_yes'], axis=1)], axis=1).to_csv('train.csv', index=False, header=False)
pd.concat([validation_data['y_yes'], validation_data.drop(['y_no', 'y_yes'], axis=1)], axis=1).to_csv('validation.csv', index=False, header=False)
pd.concat([test_data['y_yes'], test_data.drop(['y_no', 'y_yes'], axis=1)], axis=1).to_csv('test.csv', index=False, header=False)

boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train/train.csv')).upload_file('train.csv')
boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'validation/validation.csv')).upload_file('validation.csv')
~~~

## Set tuning job config
이제 hyperparmeter의 범위나, resource, storage, metric, strategy등을 정할 수 있다. 여기서 조금 특이한 점은 strategy인데, 이것은 기존에 등장하지 않은 개념이고 beysian같은 경우엔 통계학적으로 깊이가 있는 내용이기에, 추후에 별도의 포스트에서 다루기로 한다.

따라서 위 요소들을 json의 형태로 저장하면 된다.

~~~
from time import gmtime, strftime, sleep
tuning_job_name = 'xgboost-tuningjob-' + strftime("%d-%H-%M-%S", gmtime())

print (tuning_job_name)

tuning_job_config = {
    "ParameterRanges": {
      "CategoricalParameterRanges": [],
      "ContinuousParameterRanges": [
        {
          "MaxValue": "1",
          "MinValue": "0",
          "Name": "eta",
        },
        {
          "MaxValue": "10",
          "MinValue": "1",
          "Name": "min_child_weight",
        },
        {
          "MaxValue": "2",
          "MinValue": "0",
          "Name": "alpha",            
        }
      ],
      "IntegerParameterRanges": [
        {
          "MaxValue": "10",
          "MinValue": "1",
          "Name": "max_depth",
        }
      ]
    },
    "ResourceLimits": {
      "MaxNumberOfTrainingJobs": 20,
      "MaxParallelTrainingJobs": 3
    },
    "Strategy": "Bayesian",
    "HyperParameterTuningJobObjective": {
      "MetricName": "validation:auc",
      "Type": "Maximize"
    }
  }
~~~

## Set training job definition
이젠 직접적인 훈련과 관련된 요소들을 설정하는 부분이다.
주로는 input data와 output data의 s3위치나 이름을 설정하고, instance의 타입과 개수를 설정하게 된다. 마찬가지로 json의 형태로 설정하면 된다.

~~~
from sagemaker.image_uris import retrieve
training_image = retrieve(framework='xgboost', region=region, version='latest')
     
s3_input_train = 's3://{}/{}/train'.format(bucket, prefix)
s3_input_validation ='s3://{}/{}/validation/'.format(bucket, prefix)
    
training_job_definition = {
    "AlgorithmSpecification": {
      "TrainingImage": training_image,
      "TrainingInputMode": "File"
    },
    "InputDataConfig": [
      {
        "ChannelName": "train",
        "CompressionType": "None",
        "ContentType": "csv",
        "DataSource": {
          "S3DataSource": {
            "S3DataDistributionType": "FullyReplicated",
            "S3DataType": "S3Prefix",
            "S3Uri": s3_input_train
          }
        }
      },
      {
        "ChannelName": "validation",
        "CompressionType": "None",
        "ContentType": "csv",
        "DataSource": {
          "S3DataSource": {
            "S3DataDistributionType": "FullyReplicated",
            "S3DataType": "S3Prefix",
            "S3Uri": s3_input_validation
          }
        }
      }
    ],
    "OutputDataConfig": {
      "S3OutputPath": "s3://{}/{}/output".format(bucket,prefix)
    },
    "ResourceConfig": {
      "InstanceCount": 1,
      "InstanceType": "ml.m4.xlarge",
      "VolumeSizeInGB": 10
    },
    "RoleArn": role,
    "StaticHyperParameters": {
      "eval_metric": "auc",
      "num_round": "100",
      "objective": "binary:logistic",
      "rate_drop": "0.3",
      "tweedie_variance_power": "1.4"
    },
    "StoppingCondition": {
      "MaxRuntimeInSeconds": 43200
    }
}
~~~

## Launch the job
위의 기본 설정이 모두 끝나면, smclient의 create_hyper_parameter_tuning_job 메서드를 통해서 training job을 만들어준다.

이때, arguments에는 위에서 설정한 요소들을 던져주면 된다.
~~~
smclient.create_hyper_parameter_tuning_job(HyperParameterTuningJobName = tuning_job_name,
                                            HyperParameterTuningJobConfig = tuning_job_config,
                                            TrainingJobDefinition = training_job_definition)
~~~

그런데 위 셀까지 실행을 해도 노트북 상에서는 바로 나오질 않는다. 
따라서 descirbe와 관련된 명령어를 사용하거나 sagemaker의 hyperparameter tuning 파트에서 아래의 그림과 같이 훈련이 잘 되고 있는지 확인하면 된다.

## Reference
* [github](https://github.com/aws/amazon-sagemaker-examples/blob/master/hyperparameter_tuning/xgboost_direct_marketing/hpo_xgboost_direct_marketing_sagemaker_APIs.ipynb)
