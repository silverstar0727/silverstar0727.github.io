---
layout: post
title: "머신러닝 개요"
date: 2021-01-03
excerpt: "머신러닝의 전반적인 목적 및 학습의 방법"
tags: [ML Basic]
category: ML Basic
comments: False
use_math: true
---

## 머신러닝의 목적
인공지능의 필요와 같은 잡다한 얘기는 모두 공감할테니 건너뛰고 머신러닝의 목적에 대해서 설명하면, 머신러닝과 인공지능은 기본적으로 주어진 데이터의 경향성을 반영하여 미래의 인과를 설명하는 것을 목적으로 한다.

즉, X가 input으로 주어지고 Y가 output으로 주어진 데이터에 대해서 Y = F(X)인 F를 찾아 새로운 X에 대응하는 Y를 구하는 것이 목적이다.

F의 형태로는 Tree, Linear Line, Nonlinear, network 등이 있으며 이를 모델이라고 부른다.

## 예측의 종류
우리는 통계학에서 흔히 접하듯, 자료형에는 크게 연속적인 자료형, 이산적인 자료형, 범주 자료형으로 구분된다. 
이산적인 자료형은 차치하기로 하고, 우선 연속적인 자료형과 범주 자료형에 대해서만 다루기로 한다.

만약 예측하고자 하는 Y값이 연속적이라면 수치예측으로 Regression이라고 부르고, 범주형이라면 범주예측으로 Classification이라고 부른다.

## 머신러닝 모델학습 프로세스
앞에서 언급한 F를 Linear Regression으로 가정하면, $f = w_0 + w_{1}x_{1} + w_{2}x_{2}...$으로 말할 수 있고, 이러한 변수들을 어떻게 조합하는 것은 머신러닝의 핵심이자 모델의 선택에 해당한다.

이렇게 선택된 모델은 주어진 데이터를 이용한 학습(Training)으로 weights(parameters)의 값을 결정해야 한다.

* 학습의 방법
$Y = f + \epsilon$으로 참값 $Y$는 모델에 넣어서 구한 추정값 $f$와 오차인 $\epsilon$을 더한 것으로 정의된다. 여기서 $\epsilon = 0$일 때가 가장 이상적인 모델로 간주된다.
  * loss function은 개별 데이터에 대한 오차로 $\epsilon_{i} = Y_{i} - F_{i}$이고, 부호를 고려하지 않기 위해서 오차의 제곱으로 다음과 같이 표현한다. ${\epsilon}^2 = (Y_{i} - F_{i})^2$
  * cost function은 개별 데이터의 오차의 합으로 즉,  $\sum_{i}{\epsilon}^2 = \sum_{i}(Y_{i} - F_{i})^2$ 이다. 

학습은 cost function이 최소가 되도록 parameters를 결정하는 방식으로 training이 진행되고, 대개는 cost function과 loss function을 크게 구분하지는 않는다.

한편 여기서 보인 오차의 측정은 least squared estimate algorithm으로 선형회귀 이외의 모델에서는 다른 방법을 이용하여 오차를 측정하기도 한다(ex) 로지스틱회귀에서는 conjugate gradient algorithm, 인공신경망에서는 Backpropagation algorithm을 사용).

## 모델의 종류
모델은 정말 다양한 모델들이 존재한다. 나는 자연어처리(NLP) 분야를 연구하기에 비전과 같은 분야에서 유용한 모델은 잘 모르지만, 최근 제시되는 것은 Attention기반의 모델들이 SOTA(State-Of-The-Art)를 차지하고 있다. 그에 앞서 기초적으로는 앞에서 살펴본 Linear 모델, Logistic 모델, Decision Tree, 인공신경망 등이 존재한다.

* Linaer Model- least squared estimator algorithm

$$f = w_0 + w_{1}x_{1} + w_{2}x_{2}...$$

* Logistic Model - conjugated gradient algorithm

$$f = {1 \over {1 + e^{-(w_0 + w_{1}x_{1} + w_{2}x_{2}...)}}}$$

* Decision Tree Model - decision tree algorithm

$$f = \sum_{m = 1} ^n {k(m) I((x_1, x_2) \in R_m)}$$

* Artificial Neural Network Model(로지스틱 모델의 합성함수 형태) - backpropagation algorithm

$$f = {1 \over {1 + e^{-(w_0 + w_{1}({1 \over {1 + e^{-(w_01 + w_{11}x_{1} + w_{21}x_{2}...)}}}) + w_{2}({1 \over {1 + e^{-(w_02 + w_{12}x_{1} + w_{22}x_{2}...)}}})...)}}}$$

이러한 모델을 결정 짓는 것은 머신러닝의 핵심적인 요소에 해당하며, 다음 포스트에서는 선형회귀에 대한 통계적 증명 및 가설 추정과 검정에 대해 배워보자.

## Reference
[Youtube](https://www.youtube.com/watch?v=pFyFHUmxgu0&t=201s)
