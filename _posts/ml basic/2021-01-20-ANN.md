---
layout: post
title: "Artificial Neural Network()"
date: 2021-01-20
excerpt: "AI의 꽃, ANN의 등장."
tags: [ML Basic]
category: ML Basic
comments: False
use_math: true
---

## perceptron의 연속
앞서 [로지스틱 회귀](https://silverstar0727.github.io/ml%20basic/2021/01/05/%EB%A1%9C%EC%A7%80%EC%8A%A4%ED%8B%B1%ED%9A%8C%EA%B7%80/#)를 통해 이진 분류를 할 수 있다는 것을 배웠다. 
그러나 이러한 로지스틱 회귀는 단일한 직선으로 분류하는 것이기에, 직선으로 분류되지 않은 문제들에 대해서는 무용하다.

따라서, 이를 해결하기 위해 등장한 것이 ANN이다.
위에서의 로지스틱 회귀를 하나의 perceptron으로 본다면, perceptron은 다양한 독립변수들을 입력 받아 weights와 선형결합을 한 뒤, sigmoid function을 통해 0과 1사이의 결과 값을 반환한다.
ANN은 이러한 perceptron은 단일하지 않고 여러개 사용한다는 관점에서 출발한다.

언뜻보면 이러한 관점이 이해가 가지 않을 수도 있으나, 하나하나 차근차근히 이해하면 정말 별 것 없다는 것을 깨닫게 될 것이다. 
특히 아래의 그림을 보면 보다 직관적으로 이해가 간다. 



위 그림에서 수많은 독립변수인 x의 값들이 주어지고, 각 노드에 이전 layer의 모든 output이 input이 되어 들어가게 된다. 
이렇게 들어간 input은 노드에 있는 input의 개수만큼의 weights와 선형 결합을 하게되고, 0과 1 사이의 값을 sigmoid function을 통해 반환하는 것이다.

어떤가? perceptron과 정말 똑같지 않은가?

여기서 sigmoid function이외의 다른 함수를 사용한다면, 또 다른 결과의 예측이 나올 것이다. 
이러한 선형결합이 통과하는 함수를 Activation function이라고 하며, 필자가 작성한 [관련 문서](https://silverstar0727.github.io/ml%20basic/2021/01/06/Activation_Function/#)를 참조하라.

> 함수를 통과해야 하는 이유: activation function은 말 그대로 '활성화 정도'를 나타낸다. 
즉, 해당 노드의 모델 전체에서의 영향이 어느정도인지를 알려준다는 것이다
(앞으로 ANN에서의 노드는 모두 perceptron에 해당한다).

> layer는 초기 input이 노드를 통과하는 횟수가 같을 때의 노드집단을 말한다 layer는 크게 입력층, 은닉층, 출력층으로 구분된다.
입력층은 input의 독립변수의 개수와 동일해야하고, 은닉층은 상관없으나, 출력층은 원하는 class 개수만큼의 유닛으로 이루어져야 한다(그래야 결과가 class 개수만큼 0~1 사이로 나올테니 말이다).

## ANN의 매커니즘(Backpropagation)
앞서, ANN의 구조를 살펴보았다. 그렇다면 이러한 node가 뭉쳐 layer가 되고, layer가 모여 ANN이 되는데 이것을 도대체 어떻게 학습시킨단 말인가?

