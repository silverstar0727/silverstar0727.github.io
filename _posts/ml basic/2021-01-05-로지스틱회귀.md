---
layout: post
title: "로지스틱 회귀(미완)"
date: 2021-01-05
excerpt: "로지스틱 회귀의 심층적 탐구. logit transform과 likelihood를 이용한 추정 및 모델 해석"
tags: [ML Basic]
category: ML Basic
comments: False
use_math: true
---

로지스틱 회귀는 이후 Artificial Neural Network를 설명함에 있어서 가장 기초적인 Activation function으로 사용되니, 더욱 중요하다고 할 수 있다.

앞서 선형회귀를 훑어본 후 cross entropy까지 배웠는데, 선형회귀의 경우에는 Regression에 해당했다.
그러나 로지스틱 회귀는 classification에 해당한다는 점에서 차이가 있고, 앞서 배운 cross entropy를 cost function으로 사용하므로 
[이전 포스트]()를 보고 읽는 것을 추천한다.

## 배경
로지스틱회귀는 Y가 범주형 변수일 경우 이를 분류하는 것을 목적으로 만들어졌다. 
범주형 변수의 경우도 두 가지로 나뉘는데, 두 가지 값만을 가지는 경우는 이진변수이며, 여러가지 값을 가진다면 멀티 변수라고 할 수 있다.

그러나 본 포스트에서는 이진 변수에 대한 내용을 다뤄 본다.

## 가정과 정리
잔차에 대해 $E(\epsilon) = 0$이고, 따라서 $E(Y_{i}) = \beta_{0} + \beta_{1} X_{i}$라는 것이다.

또한 이진 값을 갖는 로지스틱회귀에서 $Y_{i}$는 Bernoulli random variable에 해당하여, 다음이 성립한다.

$$P(Y_{i} = 1) = \pi_{i}$$

$$P(Y_{i} = 0) = 1 - \pi_i$$

위 식을 해석하면 $Y_{i}$가 1의 값을 가질 확률을 $\pi_{i}$라고 했을 때, 0을 가질 확률은 $1-\pi_i$라는 것이다.
어려워 보이지만, 사실 당연한 얘기이다.
결국 기댓값은 $E(Y_{i}) = \pi_{i}$이다. 따라서, 기댓값은 X 값이 주어졌을 때, Y가 1의 값을 갖는 확률과 동치이다.

## 그래프의 모형과 함수 식
위와 같이 두 가지의 값만을 갖는 데이터에 적합한 함수를 구하기 위해서 sigmoid, logistic 혹은 squashing 함수라고 불리기도 하는 함수를 생각해냈다.

$$\phi(z) = {1 \over 1 + e^{-z}}$$

여기서 z는 단순 로지스틱회귀일 경우 $\beta_{0} + \beta_{1}x$로 선형회귀의 식에 해당한다. 
다중 로지스틱은 z에 다중 선형회귀식을 대입하면 된다.

해당 함수를 해석하자면 함수의 치역이 (0, 1)이며, 미분하면 exponential function의 특성에 따라, 본인 자신의 함수가 나와 아래와 같이 된다.

$${d \phi (z) \over dz} $$

$$= {1 \over 1 + e^{-z}}$$

$$= \phi(z)(1 - phi(z))$$

즉, 이러한 결과는 Gradient learning method에서 유용하게 사용될 수 있다는 장점이 있다.

또한, $E(y) = \pi((X = x)$이므로 $\pi(X = x) = {1 \over 1 + e^{-(\beta_{0} + \beta_{1}x)}}$가 성립한다.

> 이후에 배우겠지만, activation function으로는 단점이 발견되어 최근에 많이 쓰이는 함수는 아니다. 그럼에도 불구하고, 이론적인 부분을 고려한다면, 배울 가치가 있는 것은 분명하다.

## $\beta_{1}$의 해석과 Odds 그리고 logit transform
#### Odds & logit transform
로지스틱 함수에서는 $\beta_{1}$가 합성함수의 형태로 내부에 존재하기에 선형회귀와 같이 직관성이 떨어진다는 단점이 존재한다.
따라서, odds(승산)으로 이것을 설명하게 된다.

odds는 성공확률이 p라고 할 때, 실패대비 성공확률을 조건부 확률로 $p \over 1-p$로 정의된다.
이러한 odds를 로지스틱회귀에 적용하면, 1일 확률 $\pi(X = x)$에 대해 $odds = {\pi(X = x) \over 1-\pi(X = x)}$ 이다.

다시 위의 odds 식에 log를 취하여 전개하면 다음과 같이 식이 매우 간단해진다. $log(odds) = \beta_{0} + \beta_{1}x$

이러한 odds와 log를 취한 과정 전반을 합쳐 logit transform(로짓변환)이라고 하며, 이는 $\beta_{1}$의 영향에 대해 직관적으로 파악할 수 있는 길을 만들어준다.

#### $\beta_{1}$의 해석
위의 logit transform 결과를 바탕으로 $\beta_{1}$가 미치는 영향에 대해서 해석하자면, x의 단위증가량 당 log(odds)의 증가량 즉 선형 식의 기울기를 의미한다.

## Parameter 추정
