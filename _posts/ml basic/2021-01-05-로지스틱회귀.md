---
layout: post
title: "로지스틱 회귀(미완)"
date: 2021-01-05
excerpt: "로지스틱 회귀의 심층적 탐구. logit transform과 likelihood를 이용한 추정 및 모델 해석"
tags: [ML Basic]
category: ML Basic
comments: False
use_math: true
---

로지스틱 회귀는 이후 Artificial Neural Network를 설명함에 있어서 가장 기초적인 Activation function으로 사용되니, 더욱 중요하다고 할 수 있다.

앞서 선형회귀를 훑어본 후 cross entropy까지 배웠는데, 선형회귀의 경우에는 Regression에 해당했다.
그러나 로지스틱 회귀는 classification에 해당한다는 점에서 차이가 있고, 앞서 배운 cross entropy를 cost function으로 사용하므로 
[이전 포스트](https://silverstar0727.github.io/ml%20basic/2021/01/05/cross_entropy/)를 보고 읽는 것을 추천한다.

## 배경
로지스틱회귀는 Y가 범주형 변수일 경우 이를 분류하는 것을 목적으로 만들어졌다. 
범주형 변수의 경우도 두 가지로 나뉘는데, 두 가지 값만을 가지는 경우는 이진변수이며, 여러가지 값을 가진다면 멀티 변수라고 할 수 있다.

그러나 본 포스트에서는 이진 변수에 대한 내용을 다뤄 본다.

## 가정과 정리
잔차에 대해 $E(\epsilon) = 0$이고, 따라서 $E(Y_{i}) = \beta_{0} + \beta_{1} X_{i}$라는 것이다.

또한 이진 값을 갖는 로지스틱회귀에서 $Y_{i}$는 Bernoulli random variable에 해당하여, 다음이 성립한다.

$$P(Y_{i} = 1) = \pi_{i}$$

$$P(Y_{i} = 0) = 1 - \pi_i$$

위 식을 해석하면 $Y_{i}$가 1의 값을 가질 확률을 $\pi_{i}$라고 했을 때, 0을 가질 확률은 $1-\pi_i$라는 것이다.
어려워 보이지만, 사실 당연한 얘기이다.
결국 기댓값은 $E(Y_{i}) = \pi_{i}$이다. 따라서, 기댓값은 X 값이 주어졌을 때, Y가 1의 값을 갖는 확률과 동치이다.

## 그래프의 모형과 함수 식
위와 같이 두 가지의 값만을 갖는 데이터에 적합한 함수를 구하기 위해서 sigmoid, logistic 혹은 squashing 함수라고 불리기도 하는 함수를 생각해냈다.

$$\phi(z) = {1 \over 1 + e^{-z}}$$

여기서 z는 단순 로지스틱회귀일 경우 $\beta_{0} + \beta_{1}x$로 선형회귀의 식에 해당한다. 
다중 로지스틱은 z에 다중 선형회귀식을 대입하면 된다.

해당 함수를 해석하자면 함수의 치역이 (0, 1)이며, 미분하면 exponential function의 특성에 따라, 본인 자신의 함수가 나와 아래와 같이 된다.

$${d \phi (z) \over dz} $$

$$= {1 \over 1 + e^{-z}}$$

$$= \phi(z)(1 - \phi(z))$$

즉, 이러한 결과는 Gradient learning method에서 유용하게 사용될 수 있다는 장점이 있다.

또한, $E(y) = \pi((X = x)$이므로 $\pi(X = x) = {1 \over 1 + e^{-(\beta_{0} + \beta_{1}x)}}$가 성립한다.

> 이후에 배우겠지만, activation function으로는 단점이 발견되어 최근에 많이 쓰이는 함수는 아니다. 그럼에도 불구하고, 이론적인 부분을 고려한다면, 배울 가치가 있는 것은 분명하다.

## $\beta_{1}$의 해석과 Odds 그리고 logit transform
#### Odds & logit transform
로지스틱 함수에서는 $\beta_{1}$가 합성함수의 형태로 내부에 존재하기에 선형회귀와 같이 직관성이 떨어진다는 단점이 존재한다.
따라서, odds(승산)으로 이것을 설명하게 된다.

odds는 성공확률이 p라고 할 때, 실패대비 성공확률을 조건부 확률로 $p \over 1-p$로 정의된다.
이러한 odds를 로지스틱회귀에 적용하면, 1일 확률 $\pi(X = x)$에 대해 $odds = {\pi(X = x) \over 1-\pi(X = x)}$ 이다.

다시 위의 odds 식에 log를 취하여 전개하면 다음과 같이 식이 매우 간단해진다. $log(odds) = \beta_{0} + \beta_{1}x$

이러한 odds와 log를 취한 과정 전반을 합쳐 logit transform(로짓변환)이라고 하며, 이는 $\beta_{1}$의 영향에 대해 직관적으로 파악할 수 있는 길을 만들어준다.

#### $\beta_{1}$의 해석
위의 logit transform 결과를 바탕으로 $\beta_{1}$가 미치는 영향에 대해서 해석하자면, x의 단위증가량 당 log(odds)의 증가량 즉 선형 식의 기울기를 의미한다.

## Parameter 추정(Maximum Likelihood Estimation)
우리는 (앞선 포스터로)[https://silverstar0727.github.io/ml%20basic/2021/01/05/cross_entropy/#]부터 분포의 차이를 측정하는 것을 cross entropy라고 배웠다. 
로지스틱 회귀의 관심사는 $\pi(X = x)$이므로 분포에 해당하고, 따라서 로지스틱회귀에서 cross entropy는 모델을 학습하는 과정에서 데이터로 주어진 $y_{i}$값과 예측한 $\hat{y_{i}}$값의 차이로 쓰인다.
이것은 선형회귀에서의 SSE와 동일한 위상을 갖는다.

차이를 구한 이후에는 이러한 차이를 최적화 하는 방법에 대해 생각해보아야 한다.
선형회귀에서는 least mean squared algorithm을 사용한 반면 비선형 결합인 로지스틱회귀에서는 conjugated algorithm등을 사용한다(optimization theory 참조).

그러나 당장에 이러한 최적화 이론에 대해서 배우기는 힘들고, MLE(Maximum Likelihood Estimation)으로 최적화의 대상을 명확히 할 필요정도는 있다.

MLE는 한국어로는 최대우도추정이라고 하는데, 우도함수(Likelihood function)는 분포에 대한 파라미터(여기서는 $\pi$)가 얼마나 관측치에 대해 잘 부합하는 지 나타내는 함수이다.
한편, 이것은 확률함수로 부터 그 곱으로써 간단히 유도될 수 있는데, 확률함수는 아래와 같다.

$$ f_{i}(y_{i}) = \pi(x_{i})^{y_{i}} (1 - \pi(x{i}))^{1-y_{i}}$$

위 확률함수로부터 우도함수는 단순 곱을 취한 형태가 되어 다음과 같다. $L = \Pi_{i} f_{y_{i}}$ 여기서 양변에 ln을 취하면 로그우도함수가 되며, 로짓변환을 통해 아래와 같이 간단한 형태로 변환이 가능하다.

$$ln{L} = \sum_{i}{y_{i}(\beta_{0} + \beta_{1}x_1 +...) + ln(1+e^{\beta_{0} + \beta{1}x1 + ...})}$$

따라서, $ln{L}$이 최대가 되도록 $\beta$를 결정하는 것이 이제 목표가 된다. 이것은 곧 cross entropy가 최소가 되도록 하는 것과 동치이다.

## 결과 및 해석
위 과정을 토대로 완성된 모델로 예측한 결과는 0부터 1사이의 확률값으로 주어지는데 대개는 0.5를 기준으로 이상이면 1, 이하이면 0으로 분류한다.

또한, odds를 다시 도입하여 odds ratio를 통해 나머지 변수가 고정시키고 한 변수를 증가하였을때의 odds의 변화분으로 쉽게 모델을 평가할 수 있다.

## Reference
* [blog](https://curt-park.github.io/2018-09-19/loss-cross-entropy/)
* [youtube](https://www.youtube.com/channel/UCueLU1pCvFlM8Y8sth7a6RQ)
