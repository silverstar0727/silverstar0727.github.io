---
layout: post
title: "선형회귀 모델(1)"
date: 2021-01-03
excerpt: "단순 선형회귀 모델의 가정과 추정에 대한 내용"
tags: [ML Basic]
category: ML Basic
comments: False
use_math: true
---

이번에는 보다 통계적인 접근으로 선형회귀 모델을 가정으로부터 추정과 검정을 통해 그 이론적 배경을 알아보는 시간을 갖는다. 여기서 배우는 내용은 단순히 선형 회귀모델에만 적용되는 것이 아니라, ML의 기초가 되기에 더욱 중요하다.

본 포스트에서 단순 선형회귀 모델은 $$f = \beta_{0}+ \beta_{1}x_{1}$$이고 여기서의 $\beta$는 [앞선 포스트](https://silverstar0727.github.io/ml%20basic/2021/01/03/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%EA%B0%9C%EC%9A%94/#)의 $w$(weight == parameter)와 동치이다.

## 선형회귀 모델의 정의

선형회귀 모델이라고 하면 흔히들 $y = ax$와 같은 기하학적 선형 함수를 떠올리곤 하는데, 이는 설명 변수 $x$간의 선형성이 아니라, $\beta_{0} \beta_{1}$간의 선형 결합을 말한다.

즉, $y = \beta^2 x$는 선형회귀 모델이 아니나, $y = \beta x^2$는 선형회귀 모델이다.

> cf) y = f(x)에서 변수 x는 y를 "설명하는" 함수로 설명 변수라고 표현하고, 이에 따른 y를 "종속변수"라고 표현한다.

## 선형회귀의 가정
선형회귀 모델에서의 $Y = f + \epsilon$에서 $\epsilon$은 다음과 같은 가정을 갖는다.
* $\epsilon \stackrel{i.i.d}{\sim} N(0, \sigma^2)$: 이것은 $\epsilon$ 평균을 0, 표준편차를 $\sigma$로 하는 정규분포를 따른다는 것을 의미하고 이로부터 다음의 식이 도출이 가능하다.
  * $E(Y_{i}) = E(\beta_{0} + \beta_{i}x_{i} + \epsilon_{i}) = \beta_{0} + \beta_{i}x_{i} + E(\epsilon_{i}) = \beta_{0} + \beta_{1}x$
  * $Var(Y_{i}) = Var(\beta_{0} + \beta_{i}x_{i} + \epsilon_{i}) = Var(\epsilon_{i}) = \sigma^2$
  
혹시 해당 식이 이해가 가지 않는다면 고등학교의 확률과 통계 과목을 참고할 것

## 추정 방법
추정 방법은 cost function이 최소가 되도록 training을 시키는 것인데, 여기서 objective function은
$\arg \min_{\beta_{0}, \beta_{1}} \sum_{i=1}^n (Y_{i} - (\beta_{0} + \beta{i}x_{i}))^2$이고, cost function은 $Q = \sum_{i} \epsilon_{i}^2 = \sum_{i} (y_{i} - \beta_{0} - \beta_{i}x_{i})^2$ 이다. 따라서, cost function은 convex형태를 가지고 이는 전역 최적해(globally optimal solution)을 갖는다.

결국 각 parameter인 $\beta_{0}, \beta_{1}$으로 cost function을 편미분한 값이 0이 될 때의 값은 $\hat{\beta_{0}}, \hat{\beta_{1}}$이고, 아래의 이러한 식은 정규방정식(normal equation)으로 불린다.

$${\partial Q(\beta_{0}, \beta_{1}) \over \partial \beta_{0}} = 0$$

$${\partial Q(\beta_{0}, \beta_{1}) \over \partial \beta_{1}} = 0$$

$\hat{\beta_{0}}, \hat{\beta_{1}}$를 위 식에 대입하고 이를 전개하면 다음의 결과가 나온다.

$$\hat{\beta_{0}} = \bar{Y} - \hat{\beta_{1}} \bar{x}$$

$$\hat{\beta_{1}} = \sum_{i}{(x_{i} - \hat{x}) (Y_{i} - \hat{Y_{i}})  \over  (x_{i} - \hat{x})}$$

따라서 $\beta_{0}, \beta_{1}$를 선형회귀로 추정한 $\hat{\beta_{0}}, \hat{\beta_{1}}$를 통해 선형 회귀 모델식은$f = \hat{\beta_{0}} + \hat{\beta_{1}}x_{1}$이다.

그런데 우리가 모델을 생각할 때, $Y_{i} = \beta_{i} + \beta_{1}x_{1} + \epsilon_{i}$였다. 여기서 $\epsilon$은 [앞선 포스트]()에서 평균 = 0, 표준편차 = $\sigma$인 정규분포를 따른다고 했으므로 $\sigma$역시 우리가 모르는 값이다. 따라서 $\sigma^2 = $과 같이 정의 된다. 여기서 $e_{i}$는 잔차(residual)에 해당한다.

> cf) 잔차(residual)은 오차(error)와는 차이가 있다. 오차는 우리가 개선할 수 없는 실제와의 차이인 모집단의 회귀식으로부터의 차이이고, 잔차는 설명할 수 있는 오차인 표본집단으로부터 얻은 회귀식으로부터의 차이에 해당한다.

이렇게 추정된 parameter의 값은 Gauss-Markov Theorem에 의해서 BLUE(Best Linear Unviased Estimator)에 해당하는데, 다음의 두 조건을 만족하기 때문이다.
* 불편추정량: $E(\hat{\beta_{0}}) = \beta_{0}$, $E(\hat{\beta_{1}}) = \beta_{1}$
* 작은 분산

너무 글이 길어져서 여기까지 짜르고 다음 포스트에서는 검정 및 모델 평가에 대해서 배워보자. 사실 수식 적는게 진짜 힘들어 죽겠따...

## Reference
[Youtube](https://www.youtube.com/watch?v=AZ45z0eGlaw&list=PLpIPLT0Pf7IoTxTCi2MEQ94MZnHaxrP0j&index=23)
