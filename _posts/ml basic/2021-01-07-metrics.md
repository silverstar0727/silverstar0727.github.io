---
layout: post
title: "모델평가 지표: metrics(미완)"
date: 2021-01-07
excerpt: "모델을 평가하는 평가지표에 대해 알아보자"
tags: [ML Basic]
category: ML Basic
comments: False
use_math: true
---

학습이 끝나고 모델이 완성되면, 이제 이 모델을 평가해야할 차례이다. 여기서 모델이 완성된다는 것은 모델의 파라미터들(Weights)이 모두 결정된다는 것을 의미한다. 따라서 학습에 의한 파라미터가 얼마나 적합한가에 대해서 검토해야할 필요가 있다. 모델평가는 다양한 지표들에 의해서 이루어지는데, 크게 classification과 regression에서의 방법으로 구분되고 classification은 정량화되지 않은 값들에 대한 평가이기에 regression대비 조금 더 복잡하다.

따라서, 우선적으로 classification에서 어떠한 metrics가 쓰이는지 Confusion Matrix를 통해 선험적으로 보고, 구체적으로 어떠한 지표들이 실질적으로 적용되는 지 검토한 후에 regression의 다양한 지표들을 살펴보자.

## 이진분류와 Confusion matrix
#### Confusion Matrix
Confusion matrix는 classification에서 모든 경우의 수를 검토한 후, 도메인에 적합하게 이들을 조합하는 기초가 된다. 아래의 그림과 같이 4가지의 항목이 존재하며 각 항목을 설명하면 다음과 같다.
* TP: 이는 실제로 참이며, 예측도 참으로 했다는 것을 의미한다.
* FP: 이는 실제로 거짓이나, 예측을 참으로 했다는 것을 의미한다.
* FN: 이는 실제로 참이지만 예측을 거짓으로 했다는 것을 의미한다.
* TN: 이는 실제로 거짓이며, 예측도 거짓으로 했다는 것을 의미한다.

![image](https://user-images.githubusercontent.com/49096513/104833717-257f0f00-58de-11eb-8a59-c6591faf8832.png)

즉, 위 그림에 따르면, 잘못 예측한 것은 FN, FP가 되며, 이들은 각각 Type2 error, Type1 error로 불리운다.
어떠한 에러가 더 치명적인지는 적용되는 도메인에 따라 극명하게 갈리기 때문에 어떠한 평가지표를 사용할까에 대해서는 신중해야한다.

이들을 비율로써 간단하게 수치화 한 Accuracy, Specificity, Precision, Recall(Sensitivity), F1-Score 등이 있다.

* Accuracy(정확도): $TP+TN \over TP+FN+FP+TN$
* Specificity(특이도): $FP \over TN + FP$
* Precision(정밀도): $TP \over TP+FP$
* Recall(재현률), Sensitivity(민감도): $TP \over TP+FN$
* F1-score: $Precision * Recall \over Precision + Recall$

## ROC, PR, AUC
* ROC Curve
* ROC AUC
* PR Curve
* PR AUC
* ROC vs PR

## 회귀에대한 지표
* MSE
* RMSE
* MAE

## Reference
[youtube]()
[fig.1](https://glassboxmedicine.com/2019/02/17/measuring-performance-the-confusion-matrix/)
