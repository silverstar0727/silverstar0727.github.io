---
layout: post
title: "NLP 분산학습(1)"
date: 2020-11-03
excerpt: "분산학습의 필요, 종류 그리고 구조를 다룬 post"
category: Paper Review
tags: [PR]
comments: False
---

## 분산학습의 필요성
최근의 NLP 모델을 보면, 모델의 파라미터 수가 급격하게 증가하고 있는데 GPT-1은 1억 1,700만 개, GPT-2는 15억 개, GPT-3는 무려 1,750억 개의 파라미터를 사용한 것을 확인할 수 있다. 이처럼 모델의 크기가 급증하게 되면, 학습에서의 문제가 생긴다. RAM이나 GPU의 RAM에 모델을 한꺼번에 옮겨 담지 못하는 것이 그것이다. 뿐만 아니라, 대형 모델은 그만큼 대용량의 데이터를 이용하여 학습하게 되는데, 데이터까지 현존하는 최대 RAM을 가진 GPU인 V100의 32GB조차도 이를 감당할 수 없게 된다. 따라서, 모델과 데이터를 여러개의 장치에 나누어 학습하게 되는 분산 학습(Distributed Learning)의 필요성이 요구된다.

![image](https://user-images.githubusercontent.com/49096513/97958014-b79f6d00-1def-11eb-8667-877365053ea8.png)
<center>출처: 구글 이미지</center>

## 분산학습의 종류
분산학습은 모델 분산과 데이터 분산의 두개의 종류로 구분된다.
#### 모델 분산

![image](https://user-images.githubusercontent.com/49096513/97959076-ecacbf00-1df1-11eb-805d-1cadc1b13a08.png)
<center>출처: 딥러닝 분산학습 관련연구</center>

모델분산은 위 그림처럼 장비에 모델으 나누어 담는 분산으로 하나의 GPU에서 모델의 사이즈를 감당하지 못할 때 분산 처리를 한다. 그러나, 오차를 모든 디바이스에서 연계적으로 역전파 해야하기에 빈번한 통신이 발생하며 통신의 과정에서 병목이 발생할 수 있다.
#### 데이터 분산

데이터 분산은 모델의 동기화에 따라, 다시 비동기적 동기화, 동기적 동기화 그리고 둘을 혼합한 혼합 방식으로 나뉜다.

* 비동기적 동기화
  
비동기적 동기화는 복제된 여러 모델이 데이터를 이용하여 각자 그래디언트를 업데이트 하는 방식이다. 

* 동기적 동기화
  
동기적 동기화는 각 스텝마다 복제된 모델들이 학습한 그래디언트를 종합적으로 반영하여 모델을 업데이트하기 때문에 모델의 파라미터가 항상 같게 유지되지만, 학습속도가 느리다.

* 혼합
  
비동기적 동기화에서는 언급되지 않은 지연된 그래디언트 문제(tale gradient problem)가 발생하는데 각 모델이 파라미터를 업데이트하는 속도의 차이로 인해 동기화 시 서로 다른 파라미터를 업데이트 하게 된다는 내용이다. 앞선 두 방식의 단점을 보완하기 위해 두 방법을 혼합한 방식이 등장하였다. 

비동기적 동기화의 SGD와 동기적 동기화의 Asynchronous를 혼합한 ASGD방식은 비동기적 동기화 방식에서 각 모델은 학습이 끝나더라도 다른 모든 모델이 학습할때까지 기다린 후에 동기화를 진행하는 것이다.

![image](https://user-images.githubusercontent.com/49096513/97960169-3dbdb280-1df4-11eb-9947-c399e35d8874.png)
<center>출처: 딥러닝 분산학습 관련연구</center>

## 분산학습의 구조(집단 통신)
분산학습에는 파라미터 서버 구조, 집단 통신구조 그리고 앞의 둘을 혼합한 방식의 구조가 존재하지만, 이후 NLP에서 적용되는 구조는 집단 통신 구조이므로 해당 구조를 살펴 본다. 집단 통신 구조는 각 노드가 다른 모든 노드와 그래디언트를 주고 받을 수 있어야 하므로 커뮤니케이션이 중요한데 아래 그림은 이를 보완한 Undirectional Ring의 방식이다.

![image](https://user-images.githubusercontent.com/49096513/97961767-3cda5000-1df7-11eb-8795-6328fa5e64bb.png)
<center>출처: 딥러닝 분산학습 관련연구</center>


#### REFERENCE
[딥러닝 분산학습 관련연구](https://lyusungwon.github.io/assets/publications/DistributedDeepLearningTrainingOverview.pdf)
