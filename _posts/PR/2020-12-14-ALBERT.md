---
layout: post
title: "[PR]ALBERT"
date: 2020-12-14
excerpt: "ALBERT: A LITE BERT FOR SELF-SUPERVISED LEARNING OF LANGUAGE REPRESENTATIONS 논문리뷰"
category: Paper Review
tags: [PR]
comments: False
use_math: true
---

## Challenge
#### Memory Limitation
NVIDIA의 Titan X GPU는 RAM이 12GB이므로 BERT Large의 384 Sequence length의 최대 Batch Size가 1개도 되지 못한다는 것이다.
이는 한 문장에 대한 inference도 불가능하다는 것을 의미한다.
![image](https://user-images.githubusercontent.com/49096513/102694644-b8884300-4265-11eb-8126-a1256fd3d0f4.png)


#### Model Degradation
모델이 크더라도 성능이 낮아지는 경우가 존재한다는 것을 본 논문에서 지적하고 있다. 아래의 그래프는 BERT large 모델이 BERT xlarge모델보다 성능이 떨어지는 것을 보여준다. layer는 동일하난 hidden size가 두배가 크다.
![image](https://user-images.githubusercontent.com/49096513/102694638-adcdae00-4265-11eb-96c5-f7524562284b.png)


#### Training Time
본 논문에서는 계산량이 막대하다는 것을 단점으로 BERT를 학습할 때, 16개의 TPU로 4일정도 소요되었다는 것을 지적하고 있다. 최근에는 더욱 모델이 증가하여 GPT-3 모델은 훨씬 막대한 학습시간이 필요할 것으로 예상한다.

## Contribution
#### Scaling Up Representation Learning For Natural Language
input token embdding size보다 Hidden Size를 더 크게 설정하여 parameter 수를 줄이는 방법을 ALBERT에서는 선택하였다. 이는 초기 입력 벡터는 단순히 token에 대한 정보만을 담는 반면, layer를 지나면서 contextualized representation의 정보가 포함되기 때문에 추후에 늘리는 것이 함리적이라고 판단을 하였다. 

#### Cross-layer Parameter Sharing
두 번째 방법으로는 layer간 parameters의 공유이다. 각 layer의 output이 다시 input으로 들어가는 것으로 볼 수 있기도 하다. 가령 A layer와 B layer가 차례로 존재한다 했을때, 아래 단계와 같다.
* input은 A layer를 통과하여 $output_A$를 산출한다.
* $output_A$는 B layer의 input으로 들어가 $output_B$를 산출한다. 이때, $output_A$는 A의 input으로도 들어가 $output_A2$를 산출한다.

![image](https://user-images.githubusercontent.com/49096513/102694607-8aa2fe80-4265-11eb-8a72-01b6e86cb02a.png)


위와 같은 방법으로 상위 layer과 동일한 input을 넣고 이를 본 논문에서는 parameter sharing으로 표현하였다.

#### Sentence Ordering Objectives
본 논문에서는 Sentence Ordering Objectives를 Next Sentence Prediction의 대신 사용한다. NLP는 문장을 선택할 시 임의성에 의해서 Topic추론에 가까우므로 앞 뒤 문장을 변경하고 순서가 옳은 지에 대해서 학습하는 것이 정확하다고 판단하였다.

## Experiment
* batch size: 4096
* Steps: 125000
* Computing Source: TPU 64~1024 chip
* Data: wiki, book corpus 16GB

## Result
![image](https://user-images.githubusercontent.com/49096513/102694619-9b537480-4265-11eb-86e3-605322f6a9b3.png)

## Reference
[ALBERT: A LITE BERT FOR SELF-SUPERVISED LEARNING OF LANGUAGE REPRESENTATIONS](https://arxiv.org/pdf/1909.11942.pdf)
